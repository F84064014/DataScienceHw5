{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r'churn_prediction_data\\churn_prediction_data\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>551</td>\n",
       "      <td>15806307</td>\n",
       "      <td>Trevisano</td>\n",
       "      <td>720</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>114051.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>107577.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6897</td>\n",
       "      <td>15709621</td>\n",
       "      <td>Martin</td>\n",
       "      <td>682</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>62397.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113088.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4588</td>\n",
       "      <td>15619340</td>\n",
       "      <td>Palmer</td>\n",
       "      <td>672</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>119903.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>132925.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291</td>\n",
       "      <td>15620746</td>\n",
       "      <td>Napolitani</td>\n",
       "      <td>592</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>104257.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110857.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1673</td>\n",
       "      <td>15646372</td>\n",
       "      <td>Yao</td>\n",
       "      <td>753</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>120387.73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>126378.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>5345</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Yu</td>\n",
       "      <td>568</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>121079.60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>124890.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>5837</td>\n",
       "      <td>15606641</td>\n",
       "      <td>Liao</td>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>145846.07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99276.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7335</td>\n",
       "      <td>15739692</td>\n",
       "      <td>Ferri</td>\n",
       "      <td>679</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>132810.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130780.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>9552</td>\n",
       "      <td>15791373</td>\n",
       "      <td>Worsnop</td>\n",
       "      <td>715</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>118729.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95484.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>178</td>\n",
       "      <td>15739931</td>\n",
       "      <td>Ibekwe</td>\n",
       "      <td>600</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>62397.41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66315.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId     Surname  CreditScore Geography  Gender  Age  \\\n",
       "0           551    15806307   Trevisano          720     Spain    Male   38   \n",
       "1          6897    15709621      Martin          682    France  Female   54   \n",
       "2          4588    15619340      Palmer          672    France  Female   31   \n",
       "3           291    15620746  Napolitani          592     Spain  Female   40   \n",
       "4          1673    15646372         Yao          753     Spain    Male   42   \n",
       "...         ...         ...         ...          ...       ...     ...  ...   \n",
       "7995       5345    15584532          Yu          568    France  Female   35   \n",
       "7996       5837    15606641        Liao          602   Germany  Female   45   \n",
       "7997       7335    15739692       Ferri          679     Spain  Female   43   \n",
       "7998       9552    15791373     Worsnop          715    France    Male   38   \n",
       "7999        178    15739931      Ibekwe          600    France  Female   42   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          5  114051.97              2          0               1   \n",
       "1          4   62397.41              1          1               0   \n",
       "2          5  119903.67              1          1               1   \n",
       "3          4  104257.86              1          1               0   \n",
       "4          5  120387.73              1          0               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "7995       6  121079.60              2          1               1   \n",
       "7996       7  145846.07              1          1               0   \n",
       "7997       5  132810.01              1          1               0   \n",
       "7998       4  118729.45              1          0               0   \n",
       "7999       5   62397.41              1          0               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           107577.29       0  \n",
       "1           113088.60       1  \n",
       "2           132925.17       0  \n",
       "3           110857.33       0  \n",
       "4           126378.57       0  \n",
       "...               ...     ...  \n",
       "7995        124890.50       1  \n",
       "7996         99276.02       0  \n",
       "7997        130780.85       1  \n",
       "7998         95484.52       0  \n",
       "7999         66315.00       0  \n",
       "\n",
       "[8000 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "def evaluate(y_true, y_pred, show=False):\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    if show:\n",
    "        print('accuracy:', acc)\n",
    "        print('precision:', precision)\n",
    "        print('f1 score:', f1)\n",
    "\n",
    "    return acc, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class PreproPipe():\n",
    "\n",
    "    def __init__(self, target=False, normal=False):\n",
    "\n",
    "        # set target encoding\n",
    "        self.target = target\n",
    "        if not target:\n",
    "            self.GenderEncoder = LabelEncoder()\n",
    "            self.GeographyEncoder = LabelEncoder()\n",
    "        else:\n",
    "            self.GenderEncoder = dict()\n",
    "            self.GeographyEncoder = dict()\n",
    "\n",
    "        self.normal = normal\n",
    "        if normal:\n",
    "            self.CreditParam = None\n",
    "            self.AgeParam = None\n",
    "            self.BalanceParam = None\n",
    "            self.EstimatedSalaryParam = None\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        \n",
    "        self.fit(df)\n",
    "        return(self.transform(df))\n",
    "\n",
    "    def fit(self, df):\n",
    "        \n",
    "        if not self.target:\n",
    "            self.GenderEncoder.fit(df.Gender)\n",
    "            self.GeographyEncoder.fit(df.Geography)\n",
    "        else:\n",
    "            for gender in df.Gender.unique():\n",
    "                self.GenderEncoder[gender] = df.Exited[df.Gender == gender].mean()\n",
    "            for geograph in df.Geography.unique():\n",
    "                self.GeographyEncoder[geograph] = df.Exited[df.Geography == geograph].mean()\n",
    "\n",
    "        if self.normal:\n",
    "            self.CreditParam = (df.CreditScore.mean(), df.CreditScore.std())\n",
    "            self.AgeParam = (df.Age.mean(), df.Age.std())\n",
    "            self.BalanceParam = (df.Balance.mean(), df.Balance.std())\n",
    "            self.EstimatedSalaryParam = (df.EstimatedSalary.mean(), df.EstimatedSalary.std())\n",
    "\n",
    "    def transform(self, df):\n",
    "        \n",
    "        outdf = df.copy()\n",
    "\n",
    "        # encoding\n",
    "        if not self.target:\n",
    "            outdf.Gender = self.GenderEncoder.transform(df.Gender)\n",
    "            outdf.Geography = self.GeographyEncoder.transform(df.Geography)\n",
    "        else:\n",
    "            outdf.Gender = df.Gender.apply(lambda x: self.GenderEncoder[x])\n",
    "            outdf.Geography = df.Geography.apply(lambda x: self.GeographyEncoder[x])\n",
    "\n",
    "        # normalize\n",
    "        if self.normal:\n",
    "            outdf.CreditScore = df.CreditScore.apply(lambda x: (x-self.CreditParam[0])/self.CreditParam[1])\n",
    "            outdf.Age = df.Age.apply(lambda x: (x-self.AgeParam[0])/self.AgeParam[1])\n",
    "            outdf.Balance = df.Balance.apply(lambda x: (x-self.BalanceParam[0])/self.BalanceParam[1])\n",
    "            outdf.EstimatedSalary = df.Age.apply(lambda x: (x-self.EstimatedSalaryParam[0])/self.EstimatedSalaryParam[1])\n",
    "\n",
    "        # drop columns\n",
    "        drop_feature_names = ['CustomerId', 'RowNumber', 'Surname']\n",
    "        outdf = outdf.drop(drop_feature_names, axis=1)\n",
    "\n",
    "        return outdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def StratifiedSplit(df, test_size=0.1, random_state = 42, over = 0):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    X, y = np.array(df.iloc[:, :-1]), np.array(df.iloc[:, -1])\n",
    "\n",
    "    # stratified split on lable\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # sample more from minor class \n",
    "    over_idxs = np.random.choice(np.where(y_train==1)[0], over, replace=True)\n",
    "    X_train = np.r_[X_train, X_train[over_idxs]]\n",
    "    y_train = np.r_[y_train, y_train[over_idxs]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def OutlierClipping(X, y):\n",
    "\n",
    "    iforest = IsolationForest(bootstrap=True,\n",
    "                            contamination=0.04, \n",
    "                            max_features=10, \n",
    "                            max_samples=10, \n",
    "                            n_estimators=1000, \n",
    "                            n_jobs=-1,\n",
    "                            random_state=1)\n",
    "    inlier_idx = np.where(iforest.fit_predict(X) == 1)[0]\n",
    "    print(X.shape[0] - len(inlier_idx), 'outliers clipped')\n",
    "    return X[inlier_idx], y[inlier_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratifiedTest(clfs, df, transformer, niters = 5, outlier = False, over=0):\n",
    "\n",
    "    seeds = np.random.randint(1000, size=niters)\n",
    "    \n",
    "    # Preprocessing\n",
    "    df = transformer.transform(df)\n",
    "\n",
    "    final_scores = []\n",
    "\n",
    "    # foreach classifier do n trails\n",
    "    for clf in clfs:\n",
    "\n",
    "        scores = []\n",
    "        for seed in seeds:\n",
    "\n",
    "            X_train, X_test, y_train, y_test = StratifiedSplit(df, random_state=seed, over=over)\n",
    "            # outlier\n",
    "            if outlier:\n",
    "                X_train, y_train = OutlierClipping(X_train, y_train)\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            scores.append(list(evaluate(y_test, y_pred)))\n",
    "        final_scores.append(np.mean(np.array(scores), axis=0))\n",
    "\n",
    "    return pd.DataFrame(final_scores, columns=['accuracy', 'precision', 'f1_score'], index=[str(clf) for clf in clfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimplePrepro = PreproPipe()\n",
    "SimplePrepro.fit(df)\n",
    "\n",
    "targetPrepro = PreproPipe(target=True)\n",
    "targetPrepro.fit(df)\n",
    "\n",
    "normalPrepro = PreproPipe(normal=True)\n",
    "normalPrepro.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier()</th>\n",
       "      <td>0.76100</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>0.137422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegressionCV()</th>\n",
       "      <td>0.78700</td>\n",
       "      <td>0.268606</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier()</th>\n",
       "      <td>0.86025</td>\n",
       "      <td>0.770761</td>\n",
       "      <td>0.566215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier()</th>\n",
       "      <td>0.73025</td>\n",
       "      <td>0.094893</td>\n",
       "      <td>0.111987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.86725</td>\n",
       "      <td>0.765935</td>\n",
       "      <td>0.606589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\\n              importance_type='gain', interaction_constraints='',\\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\\n              min_child_weight=1, missing=nan, monotone_constraints='()',\\n              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\\n              tree_method='exact', validate_parameters=1, verbosity=None)</th>\n",
       "      <td>0.86275</td>\n",
       "      <td>0.737852</td>\n",
       "      <td>0.600092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "KNeighborsClassifier()                               0.76100   0.258486   \n",
       "LogisticRegressionCV()                               0.78700   0.268606   \n",
       "RandomForestClassifier()                             0.86025   0.770761   \n",
       "MLPClassifier()                                      0.73025   0.094893   \n",
       "LGBMClassifier()                                     0.86725   0.765935   \n",
       "XGBClassifier(base_score=0.5, booster='gbtree',...   0.86275   0.737852   \n",
       "\n",
       "                                                    f1_score  \n",
       "KNeighborsClassifier()                              0.137422  \n",
       "LogisticRegressionCV()                              0.038021  \n",
       "RandomForestClassifier()                            0.566215  \n",
       "MLPClassifier()                                     0.111987  \n",
       "LGBMClassifier()                                    0.606589  \n",
       "XGBClassifier(base_score=0.5, booster='gbtree',...  0.600092  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegressionCV(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    XGBClassifier(),\n",
    "    # RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    # LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier()</th>\n",
       "      <td>0.76275</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>0.139216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegressionCV()</th>\n",
       "      <td>0.79150</td>\n",
       "      <td>0.375881</td>\n",
       "      <td>0.072629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier()</th>\n",
       "      <td>0.85750</td>\n",
       "      <td>0.729571</td>\n",
       "      <td>0.577412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier()</th>\n",
       "      <td>0.66000</td>\n",
       "      <td>0.110325</td>\n",
       "      <td>0.136964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.85825</td>\n",
       "      <td>0.722194</td>\n",
       "      <td>0.587028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\\n              importance_type='gain', interaction_constraints='',\\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\\n              min_child_weight=1, missing=nan, monotone_constraints='()',\\n              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\\n              tree_method='exact', validate_parameters=1, verbosity=None)</th>\n",
       "      <td>0.85100</td>\n",
       "      <td>0.693203</td>\n",
       "      <td>0.568177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "KNeighborsClassifier()                               0.76275   0.265713   \n",
       "LogisticRegressionCV()                               0.79150   0.375881   \n",
       "RandomForestClassifier()                             0.85750   0.729571   \n",
       "MLPClassifier()                                      0.66000   0.110325   \n",
       "LGBMClassifier()                                     0.85825   0.722194   \n",
       "XGBClassifier(base_score=0.5, booster='gbtree',...   0.85100   0.693203   \n",
       "\n",
       "                                                    f1_score  \n",
       "KNeighborsClassifier()                              0.139216  \n",
       "LogisticRegressionCV()                              0.072629  \n",
       "RandomForestClassifier()                            0.577412  \n",
       "MLPClassifier()                                     0.136964  \n",
       "LGBMClassifier()                                    0.587028  \n",
       "XGBClassifier(base_score=0.5, booster='gbtree',...  0.568177  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegressionCV(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    XGBClassifier(),\n",
    "    # RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    # LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=targetPrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegressionCV()</th>\n",
       "      <td>0.80775</td>\n",
       "      <td>0.593384</td>\n",
       "      <td>0.273105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC()</th>\n",
       "      <td>0.80675</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.104108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier()</th>\n",
       "      <td>0.86300</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.585639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.87100</td>\n",
       "      <td>0.783096</td>\n",
       "      <td>0.615289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85700</td>\n",
       "      <td>0.882179</td>\n",
       "      <td>0.494161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy  precision  f1_score\n",
       "LogisticRegressionCV()                          0.80775   0.593384  0.273105\n",
       "SVC()                                           0.80675   0.930000  0.104108\n",
       "RandomForestClassifier()                        0.86300   0.761720  0.585639\n",
       "LGBMClassifier()                                0.87100   0.783096  0.615289\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})   0.85700   0.882179  0.494161"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "clfs = [\n",
    "    LogisticRegressionCV(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(class_weight={0:0.8, 1:0.2}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=normalPrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegressionCV()</th>\n",
       "      <td>0.79325</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.016363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC()</th>\n",
       "      <td>0.79625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier()</th>\n",
       "      <td>0.86375</td>\n",
       "      <td>0.785467</td>\n",
       "      <td>0.575974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.87175</td>\n",
       "      <td>0.787484</td>\n",
       "      <td>0.617043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85850</td>\n",
       "      <td>0.884894</td>\n",
       "      <td>0.501699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy  precision  f1_score\n",
       "LogisticRegressionCV()                          0.79325   0.186667  0.016363\n",
       "SVC()                                           0.79625   0.000000  0.000000\n",
       "RandomForestClassifier()                        0.86375   0.785467  0.575974\n",
       "LGBMClassifier()                                0.87175   0.787484  0.617043\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})   0.85850   0.884894  0.501699"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "clfs = [\n",
    "    LogisticRegressionCV(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(class_weight={0:0.8, 1:0.2}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:33:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:33:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:33:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=None,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=10...\\n                                            reg_lambda=None,\\n                                            scale_pos_weight=None,\\n                                            subsample=None, tree_method=None,\\n                                            use_label_encoder=False,\\n                                            validate_parameters=None,\\n                                            verbosity=None)),\\n                             ('rfc',\\n                              RandomForestClassifier(class_weight={0: 85,\\n                                                                   1: 15},\\n                                                     n_estimators=350,\\n                                                     n_jobs=-1)),\\n                             ('lgbm',\\n                              LGBMClassifier(class_weight={0: 0.85, 1: 0.15},\\n                                             learning_rate=0.018, max_bin=370,\\n                                             num_leaves=50))],\\n                 voting='soft', weights=[1, 1, 1])</th>\n",
       "      <td>0.86125</td>\n",
       "      <td>0.829067</td>\n",
       "      <td>0.541210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85925</td>\n",
       "      <td>0.731451</td>\n",
       "      <td>0.585514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83700</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.357545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.86125   0.829067   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85925   0.731451   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83700   0.908802   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.541210  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.585514  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.357545  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15})\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm', clf3)],\n",
    "    voting='soft',\n",
    "    weights=[1,1,1]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=None,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=10...\\n                                            reg_lambda=None,\\n                                            scale_pos_weight=None,\\n                                            subsample=None, tree_method=None,\\n                                            use_label_encoder=False,\\n                                            validate_parameters=None,\\n                                            verbosity=None)),\\n                             ('rfc',\\n                              RandomForestClassifier(class_weight={0: 85,\\n                                                                   1: 15},\\n                                                     n_estimators=350,\\n                                                     n_jobs=-1)),\\n                             ('lgbm',\\n                              LGBMClassifier(class_weight={0: 0.85, 1: 0.15},\\n                                             learning_rate=0.018, max_bin=370,\\n                                             num_leaves=50))],\\n                 voting='soft', weights=[1, 1, 1])</th>\n",
       "      <td>0.85425</td>\n",
       "      <td>0.794183</td>\n",
       "      <td>0.517441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85900</td>\n",
       "      <td>0.725477</td>\n",
       "      <td>0.588771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83750</td>\n",
       "      <td>0.880784</td>\n",
       "      <td>0.367635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.85425   0.794183   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85900   0.725477   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83750   0.880784   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.517441  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.588771  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.367635  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15})\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm', clf3)],\n",
    "    voting='soft',\n",
    "    weights=[1,1,1]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=targetPrepro)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None,\\n                                            eval_metric='logloss', gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=0.025,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constrain...\\n                                             n_estimators=200, num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.77, 1: 0.23},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 weights=[1, 1, 1.1, 1.1, 1, 1])</th>\n",
       "      <td>0.84775</td>\n",
       "      <td>0.845589</td>\n",
       "      <td>0.450845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85350</td>\n",
       "      <td>0.717535</td>\n",
       "      <td>0.563928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.85350</td>\n",
       "      <td>0.713593</td>\n",
       "      <td>0.567352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83400</td>\n",
       "      <td>0.924645</td>\n",
       "      <td>0.329170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.84775   0.845589   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85350   0.717535   \n",
       "LGBMClassifier()                                     0.85350   0.713593   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83400   0.924645   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.450845  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.563928  \n",
       "LGBMClassifier()                                    0.567352  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.329170  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Currently best          #\n",
    "# Clip Outliers 0.04(288) #\n",
    "###########################\n",
    "\n",
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.025, n_estimators=200, eval_metric = 'logloss')\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.75, 1:0.25}, n_estimators=200)\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2}, n_estimators=200)\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.77, 1:0.23}, n_estimators=200)\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='hard',\n",
    "    weights=[1, 1, 1.1, 1.1, 1, 1]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None,\\n                                            eval_metric='logloss', gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=None,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraint...\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.9, 1: 0.1},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 voting='soft', weights=[1, 1, 1, 1, 1, 1.2])</th>\n",
       "      <td>0.86625</td>\n",
       "      <td>0.834656</td>\n",
       "      <td>0.566359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85575</td>\n",
       "      <td>0.685539</td>\n",
       "      <td>0.604528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.84950</td>\n",
       "      <td>0.631572</td>\n",
       "      <td>0.632404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.85050</td>\n",
       "      <td>0.877956</td>\n",
       "      <td>0.456925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.86625   0.834656   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85575   0.685539   \n",
       "LGBMClassifier()                                     0.84950   0.631572   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.85050   0.877956   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.566359  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.604528  \n",
       "LGBMClassifier()                                    0.632404  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.456925  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss')\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.85, 1:0.15})\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2})\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.9, 1:0.1})\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='soft',\n",
    "    weights=[1,1,1,1,1,1.2]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, over=2000)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 outliers clipped\n",
      "[15:23:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[15:24:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[15:24:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[15:24:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[15:24:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=None,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=10...\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.9, 1: 0.1},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 voting='soft', weights=[1, 1, 1, 1, 1, 1.2])</th>\n",
       "      <td>0.84675</td>\n",
       "      <td>0.855409</td>\n",
       "      <td>0.440244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85275</td>\n",
       "      <td>0.713267</td>\n",
       "      <td>0.560849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83400</td>\n",
       "      <td>0.880764</td>\n",
       "      <td>0.343046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.84675   0.855409   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85275   0.713267   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83400   0.880764   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.440244  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.560849  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.343046  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.85, 1:0.15})\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2})\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.9, 1:0.1})\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='soft',\n",
    "    weights=[1,1,1,1,1,1.2]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:31:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:31:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:31:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:31:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:31:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=0.015,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=2...\\n                                             n_estimators=200, num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.9, 1: 0.1},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 weights=[1, 1, 1, 1, 1, 1.2])</th>\n",
       "      <td>0.85650</td>\n",
       "      <td>0.825450</td>\n",
       "      <td>0.515819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85650</td>\n",
       "      <td>0.737479</td>\n",
       "      <td>0.566328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83675</td>\n",
       "      <td>0.894458</td>\n",
       "      <td>0.360488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.85650   0.825450   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85650   0.737479   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83675   0.894458   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.515819  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.566328  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.360488  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.015, n_estimators=200)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.85, 1:0.15}, n_estimators=200)\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2}, n_estimators=200)\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.9, 1:0.1}, n_estimators=200)\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='hard',\n",
    "    weights=[1,1,1,1,1,1.2]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 outliers clipped\n",
      "[20:37:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[20:37:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[20:37:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[20:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "[20:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n",
      "288 outliers clipped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=0.025,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=2...\\n                                             n_estimators=200, num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.77, 1: 0.23},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 weights=[1, 1, 1.1, 1.1, 1, 1])</th>\n",
       "      <td>0.84425</td>\n",
       "      <td>0.834024</td>\n",
       "      <td>0.434052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.85325</td>\n",
       "      <td>0.727954</td>\n",
       "      <td>0.554144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.85750</td>\n",
       "      <td>0.735035</td>\n",
       "      <td>0.573017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83100</td>\n",
       "      <td>0.895658</td>\n",
       "      <td>0.314161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.84425   0.834024   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.85325   0.727954   \n",
       "LGBMClassifier()                                     0.85750   0.735035   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83100   0.895658   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.434052  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.554144  \n",
       "LGBMClassifier()                                    0.573017  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.314161  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# This is good            #\n",
    "# hard voter              #\n",
    "###########################\n",
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.025, n_estimators=200)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.75, 1:0.25}, n_estimators=200)\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2}, n_estimators=200)\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.77, 1:0.23}, n_estimators=200)\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='hard',\n",
    "    weights=[1, 1, 1.1, 1.1, 1, 1]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:40:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:40:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:40:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('xgb',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None, gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=0.025,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints=None,\\n                                            n_estimators=2...\\n                                             n_estimators=200, num_leaves=60)),\\n                             ('lgbm2',\\n                              LGBMClassifier(class_weight={0: 0.8, 1: 0.2},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=50)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.77, 1: 0.23},\\n                                             learning_rate=0.015, max_bin=370,\\n                                             n_estimators=200, num_leaves=40)),\\n                             ('lgbm4',\\n                              LGBMClassifier(learning_rate=0.015, max_bin=370,\\n                                             num_leaves=70))],\\n                 voting='soft', weights=[1, 1, 1.1, 1.1, 1, 1])</th>\n",
       "      <td>0.85450</td>\n",
       "      <td>0.835609</td>\n",
       "      <td>0.497570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.86450</td>\n",
       "      <td>0.752903</td>\n",
       "      <td>0.598330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.86550</td>\n",
       "      <td>0.761216</td>\n",
       "      <td>0.599571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.83475</td>\n",
       "      <td>0.892955</td>\n",
       "      <td>0.343044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('xgb',\\n         ...   0.85450   0.835609   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...   0.86450   0.752903   \n",
       "LGBMClassifier()                                     0.86550   0.761216   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...   0.83475   0.892955   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('xgb',\\n         ...  0.497570  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.598330  \n",
       "LGBMClassifier()                                    0.599571  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.343044  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# This is good            #\n",
    "# soft voter              #\n",
    "###########################\n",
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.025, n_estimators=200)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.75, 1:0.25}, n_estimators=200)\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2}, n_estimators=200)\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.77, 1:0.23}, n_estimators=200)\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='soft',\n",
    "    weights=[1, 1, 1.1, 1.1, 1, 1]\n",
    ")\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.86350</td>\n",
       "      <td>0.739714</td>\n",
       "      <td>0.603077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.6, 1: 0.4})</th>\n",
       "      <td>0.86075</td>\n",
       "      <td>0.778549</td>\n",
       "      <td>0.564288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85075</td>\n",
       "      <td>0.820512</td>\n",
       "      <td>0.482687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.9, 1: 0.1})</th>\n",
       "      <td>0.84475</td>\n",
       "      <td>0.830842</td>\n",
       "      <td>0.438672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.95, 1: 0.05})</th>\n",
       "      <td>0.83550</td>\n",
       "      <td>0.810106</td>\n",
       "      <td>0.381823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 accuracy  precision  f1_score\n",
       "LGBMClassifier()                                  0.86350   0.739714  0.603077\n",
       "LGBMClassifier(class_weight={0: 0.6, 1: 0.4})     0.86075   0.778549  0.564288\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})     0.85075   0.820512  0.482687\n",
       "LGBMClassifier(class_weight={0: 0.9, 1: 0.1})     0.84475   0.830842  0.438672\n",
       "LGBMClassifier(class_weight={0: 0.95, 1: 0.05})   0.83550   0.810106  0.381823"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.60, 1:0.40}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.90, 1:0.10}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.95, 1:0.05}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.2)</th>\n",
       "      <td>0.85350</td>\n",
       "      <td>0.819657</td>\n",
       "      <td>0.500311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.15)</th>\n",
       "      <td>0.85350</td>\n",
       "      <td>0.840864</td>\n",
       "      <td>0.489841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85475</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.486335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.05)</th>\n",
       "      <td>0.85425</td>\n",
       "      <td>0.902523</td>\n",
       "      <td>0.471225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.03)</th>\n",
       "      <td>0.84800</td>\n",
       "      <td>0.911966</td>\n",
       "      <td>0.429510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.02)</th>\n",
       "      <td>0.84425</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.402476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.85350   0.819657   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.85350   0.840864   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})        0.85475   0.868908   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.85425   0.902523   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.84800   0.911966   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.84425   0.920330   \n",
       "\n",
       "                                                    f1_score  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.500311  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.489841  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})       0.486335  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.471225  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.429510  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.402476  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.2, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.15, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.05, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.03, class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.02, class_weight={0:0.80, 1:0.20}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_gain_to_split is set=15, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=15, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=15, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=15, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=15, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, min_gain_to_split=15)</th>\n",
       "      <td>0.81550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, min_gain_to_split=10)</th>\n",
       "      <td>0.83000</td>\n",
       "      <td>0.947203</td>\n",
       "      <td>0.295733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, min_split_gain=5)</th>\n",
       "      <td>0.84200</td>\n",
       "      <td>0.923384</td>\n",
       "      <td>0.385094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, min_split_gain=3)</th>\n",
       "      <td>0.84300</td>\n",
       "      <td>0.924860</td>\n",
       "      <td>0.391043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, min_split_gain=1)</th>\n",
       "      <td>0.84825</td>\n",
       "      <td>0.912011</td>\n",
       "      <td>0.430469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85950</td>\n",
       "      <td>0.867708</td>\n",
       "      <td>0.513887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...   0.81550   1.000000   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...   0.83000   0.947203   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...   0.84200   0.923384   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...   0.84300   0.924860   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...   0.84825   0.912011   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})        0.85950   0.867708   \n",
       "\n",
       "                                                    f1_score  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...  0.172182  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...  0.295733  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...  0.385094  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...  0.391043  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, m...  0.430469  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})       0.513887  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, min_gain_to_split=15),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, min_gain_to_split=10),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, min_split_gain=5),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, min_split_gain=3),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, min_split_gain=1),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n_estimators=50)</th>\n",
       "      <td>0.85200</td>\n",
       "      <td>0.892937</td>\n",
       "      <td>0.460142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85725</td>\n",
       "      <td>0.882098</td>\n",
       "      <td>0.495168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n_estimators=200)</th>\n",
       "      <td>0.86000</td>\n",
       "      <td>0.858095</td>\n",
       "      <td>0.520844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n_estimators=500)</th>\n",
       "      <td>0.86500</td>\n",
       "      <td>0.824267</td>\n",
       "      <td>0.562926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n_estimators=1000)</th>\n",
       "      <td>0.86225</td>\n",
       "      <td>0.778662</td>\n",
       "      <td>0.571190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n_estimators=2000)</th>\n",
       "      <td>0.85925</td>\n",
       "      <td>0.744302</td>\n",
       "      <td>0.576024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...   0.85200   0.892937   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})        0.85725   0.882098   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...   0.86000   0.858095   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...   0.86500   0.824267   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...   0.86225   0.778662   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...   0.85925   0.744302   \n",
       "\n",
       "                                                    f1_score  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...  0.460142  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})       0.495168  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...  0.520844  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...  0.562926  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...  0.571190  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, n...  0.576024  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=50),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=100),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=200),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=500),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=1000),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.1, class_weight={0:0.80, 1:0.20}, n_estimators=2000),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.86025</td>\n",
       "      <td>0.721612</td>\n",
       "      <td>0.598304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=50)</th>\n",
       "      <td>0.86550</td>\n",
       "      <td>0.744397</td>\n",
       "      <td>0.611167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=150)</th>\n",
       "      <td>0.86150</td>\n",
       "      <td>0.735985</td>\n",
       "      <td>0.595491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=250)</th>\n",
       "      <td>0.86300</td>\n",
       "      <td>0.733584</td>\n",
       "      <td>0.605170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=500)</th>\n",
       "      <td>0.86575</td>\n",
       "      <td>0.747100</td>\n",
       "      <td>0.609768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=1000)</th>\n",
       "      <td>0.86475</td>\n",
       "      <td>0.741612</td>\n",
       "      <td>0.608759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=1500)</th>\n",
       "      <td>0.86250</td>\n",
       "      <td>0.728709</td>\n",
       "      <td>0.605229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=2000)</th>\n",
       "      <td>0.86200</td>\n",
       "      <td>0.732752</td>\n",
       "      <td>0.599723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=3000)</th>\n",
       "      <td>0.86450</td>\n",
       "      <td>0.741339</td>\n",
       "      <td>0.607429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(max_bin=5000)</th>\n",
       "      <td>0.86125</td>\n",
       "      <td>0.731995</td>\n",
       "      <td>0.596801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              accuracy  precision  f1_score\n",
       "LGBMClassifier()               0.86025   0.721612  0.598304\n",
       "LGBMClassifier(max_bin=50)     0.86550   0.744397  0.611167\n",
       "LGBMClassifier(max_bin=150)    0.86150   0.735985  0.595491\n",
       "LGBMClassifier(max_bin=250)    0.86300   0.733584  0.605170\n",
       "LGBMClassifier(max_bin=500)    0.86575   0.747100  0.609768\n",
       "LGBMClassifier(max_bin=1000)   0.86475   0.741612  0.608759\n",
       "LGBMClassifier(max_bin=1500)   0.86250   0.728709  0.605229\n",
       "LGBMClassifier(max_bin=2000)   0.86200   0.732752  0.599723\n",
       "LGBMClassifier(max_bin=3000)   0.86450   0.741339  0.607429\n",
       "LGBMClassifier(max_bin=5000)   0.86125   0.731995  0.596801"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=50, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=150, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=250, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=500, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=1000, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=1500, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=2000, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=3000, learning_rate=0.1),\n",
    "    LGBMClassifier(max_bin=5000, learning_rate=0.1),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.85825</td>\n",
       "      <td>0.737541</td>\n",
       "      <td>0.578256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(learning_rate=0.03, max_bin=1000, n_estimators=200)</th>\n",
       "      <td>0.86225</td>\n",
       "      <td>0.761745</td>\n",
       "      <td>0.583789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(learning_rate=0.015, max_bin=1000, n_estimators=200)</th>\n",
       "      <td>0.86150</td>\n",
       "      <td>0.785092</td>\n",
       "      <td>0.565203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2})</th>\n",
       "      <td>0.85475</td>\n",
       "      <td>0.857556</td>\n",
       "      <td>0.491662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.02, max_bin=1000,\\n               n_estimators=200)</th>\n",
       "      <td>0.84875</td>\n",
       "      <td>0.893394</td>\n",
       "      <td>0.441162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, learning_rate=0.017, max_bin=1000,\\n               n_estimators=200)</th>\n",
       "      <td>0.84700</td>\n",
       "      <td>0.898836</td>\n",
       "      <td>0.427971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "LGBMClassifier()                                     0.85825   0.737541   \n",
       "LGBMClassifier(learning_rate=0.03, max_bin=1000...   0.86225   0.761745   \n",
       "LGBMClassifier(learning_rate=0.015, max_bin=100...   0.86150   0.785092   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})        0.85475   0.857556   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.84875   0.893394   \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...   0.84700   0.898836   \n",
       "\n",
       "                                                    f1_score  \n",
       "LGBMClassifier()                                    0.578256  \n",
       "LGBMClassifier(learning_rate=0.03, max_bin=1000...  0.583789  \n",
       "LGBMClassifier(learning_rate=0.015, max_bin=100...  0.565203  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2})       0.491662  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.441162  \n",
       "LGBMClassifier(class_weight={0: 0.8, 1: 0.2}, l...  0.427971  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.03, n_estimators=200, max_bin=1000),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.015, n_estimators=200, max_bin=1000),\n",
    "    LGBMClassifier(class_weight={0:0.80, 1:0.20}),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.02, class_weight={0:0.80, 1:0.20}, n_estimators=200, max_bin=1000),\n",
    "    LGBMClassifier(num_leaves=31, learning_rate=0.017, class_weight={0:0.80, 1:0.20}, n_estimators=200, max_bin=1000),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=10, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier()</th>\n",
       "      <td>0.86550</td>\n",
       "      <td>0.761216</td>\n",
       "      <td>0.599571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(learning_rate=0.05, max_bin=2000, min_gain_to_split=10,\\n               num_leaves=63)</th>\n",
       "      <td>0.86075</td>\n",
       "      <td>0.782778</td>\n",
       "      <td>0.561937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.9, 1: 0.1}, learning_rate=0.02, max_bin=370,\\n               min_gain_to_split=10, num_leaves=50)</th>\n",
       "      <td>0.80450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "LGBMClassifier()                                     0.86550   0.761216   \n",
       "LGBMClassifier(learning_rate=0.05, max_bin=2000...   0.86075   0.782778   \n",
       "LGBMClassifier(class_weight={0: 0.9, 1: 0.1}, l...   0.80450   1.000000   \n",
       "\n",
       "                                                    f1_score  \n",
       "LGBMClassifier()                                    0.599571  \n",
       "LGBMClassifier(learning_rate=0.05, max_bin=2000...  0.561937  \n",
       "LGBMClassifier(class_weight={0: 0.9, 1: 0.1}, l...  0.077355  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    # RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(),\n",
    "    LGBMClassifier(max_bin = 2000, num_leaves=63, learning_rate=0.05, min_gain_to_split=10),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.02, class_weight={0:0.90, 1:0.10}, min_gain_to_split=10),\n",
    "    # LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.02, class_weight={0:0.85, 1:0.15}),\n",
    "    # LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.02, class_weight={0:0.80, 1:0.20}),\n",
    "    # LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.02, class_weight={0:0.75, 1:0.25}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('rfc',\\n                              XGBClassifier(base_score=None, booster=None,\\n                                            colsample_bylevel=None,\\n                                            colsample_bynode=None,\\n                                            colsample_bytree=None,\\n                                            eval_metric='logloss', gamma=None,\\n                                            gpu_id=None, importance_type='gain',\\n                                            interaction_constraints=None,\\n                                            learning_rate=0.2,\\n                                            max_delta_step=None, max_depth=None,\\n                                            min_child_weight=None, missing=nan,\\n                                            monotone_constraints...\\n                              LGBMClassifier(class_weight={0: 0.9, 1: 0.1},\\n                                             learning_rate=0.05, max_bin=1000,\\n                                             min_gain_to_split=4,\\n                                             n_estimators=200)),\\n                             ('lgbm3',\\n                              LGBMClassifier(class_weight={0: 0.87, 1: 0.13},\\n                                             learning_rate=0.02, max_bin=1000,\\n                                             min_gain_to_split=4,\\n                                             n_estimators=200)),\\n                             ('lgbm4',\\n                              LGBMClassifier(boosting_type='dart',\\n                                             learning_rate=0.05,\\n                                             max_bin=2000))],\\n                 voting='soft', weights=[1.2, 1, 1, 1.1, 1.4])</th>\n",
       "      <td>0.8555</td>\n",
       "      <td>0.897269</td>\n",
       "      <td>0.480644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(class_weight={0: 85, 1: 15}, n_estimators=350, n_jobs=-1)</th>\n",
       "      <td>0.8565</td>\n",
       "      <td>0.702307</td>\n",
       "      <td>0.594605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier(class_weight={0: 0.85, 1: 0.15}, learning_rate=0.018,\\n               max_bin=370, num_leaves=50)</th>\n",
       "      <td>0.8445</td>\n",
       "      <td>0.894781</td>\n",
       "      <td>0.412602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    accuracy  precision  \\\n",
       "VotingClassifier(estimators=[('rfc',\\n         ...    0.8555   0.897269   \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...    0.8565   0.702307   \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...    0.8445   0.894781   \n",
       "\n",
       "                                                    f1_score  \n",
       "VotingClassifier(estimators=[('rfc',\\n         ...  0.480644  \n",
       "RandomForestClassifier(class_weight={0: 85, 1: ...  0.594605  \n",
       "LGBMClassifier(class_weight={0: 0.85, 1: 0.15},...  0.412602  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.2, n_estimators=200, eval_metric = 'logloss')\n",
    "# clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf2 = XGBClassifier(use_label_encoder=False, learning_rate = 0.2, n_estimators=350, eval_metric = 'logloss')\n",
    "clf3 = LGBMClassifier(max_bin=1000, learning_rate=0.05, class_weight={0:0.91, 1:0.09}, n_estimators=200, min_gain_to_split=5)\n",
    "clf4 = LGBMClassifier(max_bin=1000, learning_rate=0.05, class_weight={0:0.9, 1:0.1}, n_estimators=200, min_gain_to_split=4)\n",
    "clf5 = LGBMClassifier(max_bin=1000, learning_rate=0.02, class_weight={0:0.87, 1:0.13}, n_estimators=200, min_gain_to_split=4)\n",
    "clf6 = LGBMClassifier(max_bin=2000, learning_rate=0.05,  boosting_type='dart')\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    # estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    estimators=[('rfc', clf2),('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='soft',\n",
    "    weights=[1.2 ,1, 1, 1.1, 1.4]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "clfs = [\n",
    "    ensemble_clf,\n",
    "    RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1),\n",
    "    LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.018, class_weight={0:0.85, 1:0.15}),\n",
    "    ]\n",
    "\n",
    "table = StratifiedTest(clfs, df, transformer=SimplePrepro, outlier=False, over=1000)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 10)\n",
      "[LightGBM] [Warning] min_gain_to_split is set=5, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=5\n",
      "VotingClassifier(estimators=[('rfc',\n",
      "                              XGBClassifier(base_score=None, booster=None,\n",
      "                                            colsample_bylevel=None,\n",
      "                                            colsample_bynode=None,\n",
      "                                            colsample_bytree=None,\n",
      "                                            eval_metric='logloss', gamma=None,\n",
      "                                            gpu_id=None, importance_type='gain',\n",
      "                                            interaction_constraints=None,\n",
      "                                            learning_rate=0.2,\n",
      "                                            max_delta_step=None, max_depth=None,\n",
      "                                            min_child_weight=None, missing=nan,\n",
      "                                            monotone_constraints...\n",
      "                              LGBMClassifier(class_weight={0: 0.9, 1: 0.1},\n",
      "                                             learning_rate=0.05, max_bin=1000,\n",
      "                                             min_gain_to_split=4,\n",
      "                                             n_estimators=200)),\n",
      "                             ('lgbm3',\n",
      "                              LGBMClassifier(class_weight={0: 0.87, 1: 0.13},\n",
      "                                             learning_rate=0.02, max_bin=1000,\n",
      "                                             min_gain_to_split=4,\n",
      "                                             n_estimators=200)),\n",
      "                             ('lgbm4',\n",
      "                              LGBMClassifier(boosting_type='dart',\n",
      "                                             learning_rate=0.05,\n",
      "                                             max_bin=2000))],\n",
      "                 voting='soft', weights=[1.2, 1, 1, 1.1, 1.4])\n"
     ]
    }
   ],
   "source": [
    "def ToUpload(clf, df, transformer, outlier=False, refit=True, over=0):\n",
    "\n",
    "    if refit:\n",
    "        df = transformer.transform(df)\n",
    "        X,y = np.array(df.iloc[:, :-1]), np.array(df.iloc[:,-1])\n",
    "\n",
    "        # sample more from minor class \n",
    "        if over:\n",
    "            over_idxs = np.random.choice(np.where(y==1)[0], over, replace=True)\n",
    "            X = np.r_[X, X[over_idxs]]\n",
    "            y = np.r_[y, y[over_idxs]]\n",
    "\n",
    "\n",
    "        if outlier:\n",
    "            X, y = OutlierClipping(X, y)\n",
    "        print(X.shape)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "    # upload part\n",
    "    df_upload = pd.read_csv(r'churn_prediction_data\\churn_prediction_data\\test.csv')\n",
    "\n",
    "    rownum = df_upload['RowNumber']\n",
    "    upload_pred = clf.predict(transformer.transform(df_upload))\n",
    "    pd.concat([rownum, pd.Series(upload_pred, name='Exited')], axis=1).to_csv(r'output.csv')\n",
    "\n",
    "ToUpload(ensemble_clf, df, SimplePrepro, outlier=False, refit=True, over=1000)\n",
    "print(ensemble_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.2, n_estimators=200, eval_metric = 'logloss')\n",
    "# # clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "# clf2 = XGBClassifier(use_label_encoder=False, learning_rate = 0.2, n_estimators=350, eval_metric = 'logloss')\n",
    "# clf3 = LGBMClassifier(max_bin=1000, learning_rate=0.05, class_weight={0:0.91, 1:0.09}, n_estimators=200, min_gain_to_split=5)\n",
    "# clf4 = LGBMClassifier(max_bin=1000, learning_rate=0.05, class_weight={0:0.9, 1:0.1}, n_estimators=200, min_gain_to_split=5)\n",
    "# clf5 = LGBMClassifier(max_bin=1000, learning_rate=0.02, n_estimators=200, min_gain_to_split=4)\n",
    "# clf6 = LGBMClassifier(max_bin=2000, learning_rate=0.05,  boosting_type='dart')\n",
    "\n",
    "# ensemble_clf = VotingClassifier(\n",
    "#     # estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "#     estimators=[('rfc', clf2),('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "#     voting='soft',\n",
    "#     weights=[1.2 ,1, 1, 1.1, 1.2]\n",
    "# )\n",
    "\n",
    "clf1 = XGBClassifier(use_label_encoder=False, learning_rate = 0.025, n_estimators=200)\n",
    "clf2 = RandomForestClassifier(n_estimators = 350, class_weight={0:85, 1:15}, n_jobs=-1)\n",
    "clf3 = LGBMClassifier(max_bin=370, num_leaves=60, learning_rate=0.015, class_weight={0:0.75, 1:0.25}, n_estimators=200)\n",
    "clf4 = LGBMClassifier(max_bin=370, num_leaves=50, learning_rate=0.015, class_weight={0:0.80, 1:0.2}, n_estimators=200)\n",
    "clf5 = LGBMClassifier(max_bin=370, num_leaves=40, learning_rate=0.015, class_weight={0:0.77, 1:0.23}, n_estimators=200)\n",
    "clf6 = LGBMClassifier(max_bin=370, num_leaves=70, learning_rate=0.015)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('xgb', clf1), ('rfc', clf2), ('lgbm1', clf3), ('lgbm2', clf4), ('lgbm3', clf5), ('lgbm4', clf6)],\n",
    "    voting='soft',\n",
    "    weights=[1, 1, 1.1, 1.1, 1, 1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:434: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:434: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:434: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:434: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Counter({0: 1789, 1: 211})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:434: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.866250</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.578740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.525862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.539062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.845625</td>\n",
       "      <td>0.773973</td>\n",
       "      <td>0.477801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.858125</td>\n",
       "      <td>0.787356</td>\n",
       "      <td>0.546906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision  f1_score\n",
       "0  0.866250   0.807692  0.578740\n",
       "1  0.862500   0.884058  0.525862\n",
       "2  0.852500   0.741935  0.539062\n",
       "3  0.845625   0.773973  0.477801\n",
       "4  0.858125   0.787356  0.546906"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "def SKFUpload(clf, df, transformer,kf=3, outlier=False, over=0, weight_='unif'):\n",
    "\n",
    "    seeds = np.random.randint(1000)\n",
    "\n",
    "    df = transformer.transform(df)\n",
    "    X, y = np.array(df.iloc[:, :-1]), np.array(df.iloc[:, -1])\n",
    "\n",
    "    # load upload file\n",
    "    df_upload = pd.read_csv(r'churn_prediction_data\\churn_prediction_data\\test.csv')\n",
    "    rownum = df_upload['RowNumber']\n",
    "    \n",
    "    results = []\n",
    "    upload_preds = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kf)\n",
    "    for idx, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        # sample more from minor class \n",
    "        if idx % 2 == 0:\n",
    "            over_idxs = np.random.choice(np.where(y_train==1)[0], over, replace=True)\n",
    "            X_train = np.r_[X_train, X_train[over_idxs]]\n",
    "            y_train = np.r_[y_train, y_train[over_idxs]]\n",
    "\n",
    "        # fit and show result\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results.append(evaluate(y_test, y_pred))\n",
    "\n",
    "        # upload part\n",
    "        upload_preds.append(clf.predict_proba(transformer.transform(df_upload)))\n",
    "\n",
    "\n",
    "    upload_prob = np.zeros((len(rownum), 2))\n",
    "    if weight_ == 'unif':\n",
    "        weights = [1 for result in results]\n",
    "    elif weight_ == 'f1':\n",
    "        weights = [result[2] for result in results]\n",
    "    elif weight_ == 'precision':\n",
    "        weights = [result[1] for result in results]\n",
    "\n",
    "    for upload_pred, weight in zip(upload_preds, weights):\n",
    "        upload_prob += weight * upload_pred\n",
    "\n",
    "\n",
    "    upload_result = np.argmax(upload_prob, axis=1)\n",
    "    print(Counter(upload_result))\n",
    "    pd.concat([rownum, pd.Series(upload_result, name='Exited')], axis=1).to_csv(r'output.csv')\n",
    "\n",
    "    return pd.DataFrame(results, columns=['accuracy', 'precision', 'f1_score'], index=np.arange(kf))\n",
    "\n",
    "\n",
    "table = SKFUpload(ensemble_clf, df, SimplePrepro, over=1000, kf=5, weight_='precision')\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
